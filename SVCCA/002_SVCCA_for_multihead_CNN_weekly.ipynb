{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVCCA for weekly forecast with multihead CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "import cca_core\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank Correlation of SVCCA mean model correlation (net similarity) and Transferability (Transfer Performance increase)\n",
    "def rank_pearson_corr(transferability, svcca_corr_all):\n",
    "    # spearman roh rank-correlation\n",
    "    spearman_corr, spearman_pvalue = stats.spearmanr(transferability, svcca_corr_all)\n",
    "    print(\"Spearman Correlation:\", spearman_corr)\n",
    "    print(\"Spearman p_value:\", spearman_pvalue)\n",
    "    print(\"\")\n",
    "    # kendall tau rank-correlation\n",
    "    kendall_tau, kendall_p_value = stats.kendalltau(transferability, svcca_corr_all)\n",
    "    print(\"Kendall Correlation:\", kendall_tau)\n",
    "    print(\"Kendall p_value:\", kendall_p_value)\n",
    "    print(\"\")\n",
    "    # pearson linear correlation for comparison\n",
    "    pearson_corr, pearson_pvalue = stats.pearsonr(transferability, svcca_corr_all)\n",
    "    print(\"Pearson Correlation:\", pearson_corr)\n",
    "    print(\"Pearson p_value:\", pearson_pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load python file to compare SVCCA results with results saved in csv / sanity check\n",
    "import SVCCA_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_similarities = pd.read_csv('../temp/net_similarity_weekly.csv')\n",
    "\n",
    "# net_similarities visualization\n",
    "print(\"Net Similarity:\")\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "net_sim_visual = net_similarities.style.background_gradient(cmap=cm)\n",
    "net_sim_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with transferability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transferability = [\n",
    "    [0,-0.0026408,0.0053692,-0.0118591,0.1293025,0.0212494],\n",
    "    [0.0657007,0,-0.0159039,-0.0119574,0.1552900,0.0607466],\n",
    "    [0.0504179,0.0727003,0,-0.0820202,0.0724330,0.0207745],\n",
    "    [0.0448396,0.0131069,-0.0678191,0,0.0717826,0.0415678],\n",
    "    [0.0517473,0.0129677,-0.0214962,-0.0590644,0,0.0530523],\n",
    "    [0.0448200,0.0059016,-0.0314071,-0.0415958,0.2059353,0]\n",
    "]\n",
    "transferability = pd.DataFrame(transferability)\n",
    "# transferability\n",
    "#sns.heatmap(transferability, cmap='Greens', annot=True)\n",
    "sns.set(font_scale=1.1)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "ax.set_title('Transferability', fontsize=26)\n",
    "sns.heatmap(transferability, annot=True, fmt=\"f\", linewidths=.5, ax=ax,robust=True, cmap='Greens') #RdBu_r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten dataframes to array for correlation calculation\n",
    "\n",
    "# Net Similarity data frame\n",
    "net_sim_array = np.ravel(net_similarities.values)\n",
    "\n",
    "# Transferability data frame\n",
    "transfer_array = np.ravel(transferability.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank (Spearman) and Pearson Correlation \n",
    "# without transferability=0 and net_similarity=1 for all branches / diagonal in matrix\n",
    "print('Rank and Pearson Correlation without transferability=0 and net_similarity=1 for all branches')\n",
    "print(\"\")\n",
    "\n",
    "relevant_indices = [1,2,3,4,5,6,8,9,10,11,12,13,15,16,17,18,19,20,22,23,24,25,26,27,29,30,31,32,33,34]\n",
    "# len(transfer_array[relevant_indices])\n",
    "#rank_pearson_corr(transfer_array, net_sim_array)\n",
    "rank_pearson_corr(transfer_array[relevant_indices], net_sim_array[relevant_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value to test for non-correlation:\n",
    " \n",
    "The two-sided p-value for a hypothesis test whose null hypothesis is that two sets of data are uncorrelated, has same dimension as rho.\n",
    "\n",
    "\n",
    "p-value ≤ α: The correlation is statistically significant (usually α = 0.05 or 0.01)\n",
    "\n",
    "If the p-value is less than or equal to the significance level, then you can conclude that the correlation is different from 0.\n",
    "\n",
    "p-value >> 0.05 or 0.01, H0 can't be disproved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for correlation example\n",
    "\n",
    "a = [1,2,3,4,5,6]\n",
    "b = [2,4,6,8,10,12]\n",
    "\n",
    "rank_pearson_corr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [4,1,5,-1,-10,14]\n",
    "rank_pearson_corr(a,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# head-wise comparison of net similarity and transferability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for calculating SVCCA for all branches\n",
    "\n",
    "def SVCCA(activations1, activations2, layer_number):\n",
    "    # SVCCA different x\n",
    "    # print(\"Results using SVCCA keeping 30 dims\")\n",
    "    # load activations\n",
    "    acts1 = np.genfromtxt(activations1 + str(layer_number) + '.csv', delimiter=',')\n",
    "    acts2 = np.genfromtxt(activations2 + str(layer_number) + '.csv', delimiter=',')\n",
    "\n",
    "    # Mean subtract activations\n",
    "    cacts1 = acts1 - np.mean(acts1, axis=0, keepdims=True)\n",
    "    cacts2 = acts2 - np.mean(acts2, axis=0, keepdims=True)\n",
    "\n",
    "    # Perform SVD\n",
    "    U1, s1, V1 = np.linalg.svd(cacts1, full_matrices=False)\n",
    "    U2, s2, V2 = np.linalg.svd(cacts2, full_matrices=False)\n",
    "\n",
    "    svacts1 = np.dot(s1[:30] * np.eye(30), V1[:30])  # default: np.dot(s1[:20]*np.eye(20), V1[:20]), 49\n",
    "    # can also compute as svacts1 = np.dot(U1.T[:20], cacts1)\n",
    "    svacts2 = np.dot(s2[:30] * np.eye(30), V2[:30])  # default: np.dot(s2[:20]*np.eye(20), V2[:20]), 49\n",
    "    # can also compute as svacts1 = np.dot(U2.T[:20], cacts2)\n",
    "\n",
    "    svcca_results = cca_core.get_cca_similarity(svacts1, svacts2, epsilon=1e-10, verbose=False)  # 1e-10\n",
    "    # print(\"Layer Number:\", layer_number)\n",
    "    # print(\"SVCCA Correlation Coefficient:\", np.mean(svcca_results[\"cca_coef1\"]))\n",
    "    return np.mean(svcca_results[\"cca_coef1\"])  # , acts1, cacts1, U1, s1, V1, svacts1\n",
    "\n",
    "\n",
    "def calaculate_mean_model_correlation(activation1, activation2, conv_layers):\n",
    "    # list for storing layer correlations\n",
    "    layer_corr = []\n",
    "    # calculate and save SVCCA correlation between all layers of two base models\n",
    "    for conv in conv_layers:\n",
    "        corr = SVCCA(activation1, activation2, conv)  # , acts1, cacts1, U1, s1, V1, svacts1\n",
    "        layer_corr.append(corr)\n",
    "    # calculate mean model correlation of stored layer correlation\n",
    "    mean_model_corr = np.mean(layer_corr)\n",
    "    # print(activation1[-13:-8] + activation2[-14:-8] + \" Mean Model Correlation:\", mean_model_corr)\n",
    "    return mean_model_corr\n",
    "\n",
    "\n",
    "def calculate_SVCCA_for_all_branches(conv_layers):\n",
    "    # list of target branch activations on target branch input data\n",
    "    act_targets = ['activations/weekly/m1_x1_weekly/',\n",
    "                   'activations/weekly/m2_x2_weekly/',\n",
    "                   'activations/weekly/m3_x3_weekly/',\n",
    "                   'activations/weekly/m4_x4_weekly/',\n",
    "                   'activations/weekly/m5_x5_weekly/',\n",
    "                   'activations/weekly/m6_x6_weekly/']\n",
    "\n",
    "    # list to store mean model correlations\n",
    "    model_correlations = []\n",
    "\n",
    "    # list of activations on target branch input data\n",
    "    act1_list = ['activations/weekly/m1_x1_weekly/',\n",
    "                 'activations/weekly/m2_x1_weekly/',\n",
    "                 'activations/weekly/m3_x1_weekly/',\n",
    "                 'activations/weekly/m4_x1_weekly/',\n",
    "                 'activations/weekly/m5_x1_weekly/',\n",
    "                 'activations/weekly/m6_x1_weekly/']\n",
    "\n",
    "    for act1 in act1_list:\n",
    "        cor1 = calaculate_mean_model_correlation(act_targets[0], act1, conv_layers)\n",
    "        model_correlations.append(cor1)\n",
    "\n",
    "    act2_list = ['activations/weekly/m1_x2_weekly/',\n",
    "                 'activations/weekly/m2_x2_weekly/',\n",
    "                 'activations/weekly/m3_x2_weekly/',\n",
    "                 'activations/weekly/m4_x2_weekly/',\n",
    "                 'activations/weekly/m5_x2_weekly/',\n",
    "                 'activations/weekly/m6_x2_weekly/']\n",
    "\n",
    "    for act2 in act2_list:\n",
    "        cor2 = calaculate_mean_model_correlation(act_targets[1], act2, conv_layers)\n",
    "        model_correlations.append(cor2)\n",
    "\n",
    "    act3_list = ['activations/weekly/m1_x3_weekly/',\n",
    "                 'activations/weekly/m2_x3_weekly/',\n",
    "                 'activations/weekly/m3_x3_weekly/',\n",
    "                 'activations/weekly/m4_x3_weekly/',\n",
    "                 'activations/weekly/m5_x3_weekly/',\n",
    "                 'activations/weekly/m6_x3_weekly/']\n",
    "\n",
    "    for act3 in act3_list:\n",
    "        cor3 = calaculate_mean_model_correlation(act_targets[2], act3, conv_layers)\n",
    "        model_correlations.append(cor3)\n",
    "\n",
    "    act4_list = ['activations/weekly/m1_x4_weekly/',\n",
    "                 'activations/weekly/m2_x4_weekly/',\n",
    "                 'activations/weekly/m3_x4_weekly/',\n",
    "                 'activations/weekly/m4_x4_weekly/',\n",
    "                 'activations/weekly/m5_x4_weekly/',\n",
    "                 'activations/weekly/m6_x4_weekly/']\n",
    "\n",
    "    for act4 in act4_list:\n",
    "        cor4 = calaculate_mean_model_correlation(act_targets[3], act4, conv_layers)\n",
    "        model_correlations.append(cor4)\n",
    "\n",
    "    act5_list = ['activations/weekly/m1_x5_weekly/',\n",
    "                 'activations/weekly/m2_x5_weekly/',\n",
    "                 'activations/weekly/m3_x5_weekly/',\n",
    "                 'activations/weekly/m4_x5_weekly/',\n",
    "                 'activations/weekly/m5_x5_weekly/',\n",
    "                 'activations/weekly/m6_x5_weekly/']\n",
    "\n",
    "    for act5 in act5_list:\n",
    "        cor5 = calaculate_mean_model_correlation(act_targets[4], act5, conv_layers)\n",
    "        model_correlations.append(cor5)\n",
    "\n",
    "    act6_list = ['activations/weekly/m1_x6_weekly/',\n",
    "                 'activations/weekly/m2_x6_weekly/',\n",
    "                 'activations/weekly/m3_x6_weekly/',\n",
    "                 'activations/weekly/m4_x6_weekly/',\n",
    "                 'activations/weekly/m5_x6_weekly/',\n",
    "                 'activations/weekly/m6_x6_weekly/']\n",
    "\n",
    "    for act6 in act6_list:\n",
    "        cor6 = calaculate_mean_model_correlation(act_targets[5], act6, conv_layers)\n",
    "        model_correlations.append(cor6)\n",
    "\n",
    "    # transform into array and split it into array of 6\n",
    "    model_correlations_array = np.array(model_correlations)\n",
    "    model_correlations_array = np.split(model_correlations_array, 6)\n",
    "    # transform into data frame\n",
    "    model_correlations_df = pd.DataFrame(model_correlations_array)\n",
    "    # restructure data frame\n",
    "    model_correlations_df = model_correlations_df.T\n",
    "    return model_correlations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check if we select the right layers of head_1 we load a example CNN model and check the enumeration of layers\n",
    "from keras.models import load_model\n",
    "example_model = load_model('../models/pretrained/branch1_cnn_weekly.h5')\n",
    "print(example_model.layers[4].name)\n",
    "print(example_model.layers[8].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# head 1 (revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calaculate net similarity just for the first head of CNN model (revenue head) which consists of layer 4 and 8\n",
    "head_1_layers = [4, 8]\n",
    "head_1_net_sim = calculate_SVCCA_for_all_branches(head_1_layers)\n",
    "head_1_visual = head1_net_sim.style.background_gradient(cmap=cm)\n",
    "head_1_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional layers of head_1 (revenue) are very similar! Mean model correlation > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten data frame\n",
    "head_1_net_sim_array = np.ravel(head_1_net_sim.values)\n",
    "\n",
    "# calculate correlation between net similarity of head 1 and transferability\n",
    "rank_pearson_corr(head_1_net_sim_array[relevant_indices], transfer_array[relevant_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# head 2 (month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calaculate net similarity just for the first head of CNN model (month head) which consists of layer 5 and 9\n",
    "head_2_layers = [5, 9]\n",
    "head_2_net_sim = calculate_SVCCA_for_all_branches(head_2_layers)\n",
    "head_2_visual = plot_net_sim(head2_net_sim)\n",
    "head_2_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head 2 (month) of the models differs and shows a symetric behaviour in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten data frame\n",
    "head_2_net_sim_array = np.ravel(head_2_net_sim.values)\n",
    "\n",
    "# calculate correlation between net similarity of head 2 and transferability\n",
    "rank_pearson_corr(head_2_net_sim_array[relevant_indices], transfer_array[relevant_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# head 3 (weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calaculate net similarity just for the third head of CNN model (weekday head) which consists of layer 6 and 10\n",
    "head_3_layers = [6, 10]\n",
    "head_3_net_sim = calculate_SVCCA_for_all_branches(head_3_layers)\n",
    "head_3_visual = plot_net_sim(head_3_net_sim)\n",
    "head_3_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional layers of Head_3 (weekday) have a symmetric relationship in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten data frame\n",
    "head_3_net_sim_array = np.ravel(head_3_net_sim.values)\n",
    "\n",
    "# calculate correlation between net similarity of head 3 and transferability\n",
    "rank_pearson_corr(head_3_net_sim_array[relevant_indices], transfer_array[relevant_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# head 4 (year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calaculate net similarity just for the fourth head of CNN model (year) which consists of layer 7 and 11\n",
    "head_4_layers = [7, 11]\n",
    "head_4_net_sim = calculate_SVCCA_for_all_branches(head_4_layers)\n",
    "head_4_visual = plot_net_sim(head_4_net_sim)\n",
    "head_4_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net similarity for Head 4 (year) also shows a symetric relationship in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten data frame\n",
    "head_4_net_sim_array = np.ravel(head_4_net_sim.values)\n",
    "\n",
    "# calculate correlation between net similarity of head 4 and transferability\n",
    "rank_pearson_corr(head_4_net_sim_array[relevant_indices], transfer_array[relevant_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall there is no significant correlation between the net similarity of the distinct heads and the transfer performance delta\n",
    "\n",
    "# Row-wise comparison of net similarity and transferability\n",
    "\n",
    "# row 1 (layer 4, 5, 6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calaculate net similarity just for the first row which consists of layer 4, 5, 6, 7\n",
    "row_1_layers = [4, 5, 6, 7]\n",
    "row_1_net_sim = calculate_SVCCA_for_all_branches(row_1_layers)\n",
    "row_1_visual = plot_net_sim(row_1_net_sim)\n",
    "row_1_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten data frame\n",
    "row_1_net_sim_array = np.ravel(row_1_net_sim.values)\n",
    "\n",
    "# calculate correlation between net similarity of convolutional row 1 and transferability\n",
    "rank_pearson_corr(row_1_net_sim_array[relevant_indices], transfer_array[relevant_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# row 2 (layers 8, 9, 10, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calaculate net similarity just for the first row which consists of layer 8, 9, 10, 11\n",
    "row_2_layers = [8, 9, 10, 11]\n",
    "row_2_net_sim = calculate_SVCCA_for_all_branches(row_2_layers)\n",
    "row_2_visual = plot_net_sim(row_2_net_sim)\n",
    "row_2_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten data frame\n",
    "row_2_net_sim_array = np.ravel(row_2_net_sim.values)\n",
    "\n",
    "# calculate correlation between net similarity of convolutional row 2 and transferability\n",
    "rank_pearson_corr(row_2_net_sim_array[relevant_indices], transfer_array[relevant_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall there is no significant correlation between the net similarity of the two rows of the CNN models and the transfer performance delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
